{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "import h5py, os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from functions import transforms as T\n",
    "from torch.nn import functional as F\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  # check whether a GPU is available\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    # mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    mask_func = MaskFunc(center_fractions=center_fract, accelerations=acc)\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), rawdata_und.squeeze(0), masks.squeeze(0), norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    limit = 60\n",
    "    \n",
    "        \n",
    "    l = sorted(os.listdir(data_path))\n",
    "    print(len(l))\n",
    "    \n",
    "    for i in range(len(train_and_val)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        if i == 0 : la = l[:limit]\n",
    "        else : la = l[limit:]\n",
    "        print(len(la))\n",
    "    \n",
    "        for fname in la:\n",
    "                \n",
    "            subject_data_path = os.path.join(data_path, fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue\n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            if i == 1:\n",
    "                data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(0, num_slice)]\n",
    "            else:\n",
    "                data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(17, 22)]\n",
    "    \n",
    "    return data_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "60\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#PREPARE THE DATA \n",
    "data_list = load_data_path('/data/local/NC2019MRI/train')\n",
    "# slices, height, width = input_k.shape()\n",
    "\n",
    "acc = [4,8]\n",
    "cen_fract = [0.08, 0.04]\n",
    "# acc = [4]\n",
    "# cen_fract = [0.08]\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 10 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "\n",
    "def my_collate(batch):\n",
    "    batch_len = len(batch)\n",
    "    data = torch.ones(batch_len, 1, 320, 320)\n",
    "    target_list = torch.ones(batch_len, 1, 320, 320)\n",
    "    \n",
    "    for batch_value in range(len(batch)):\n",
    "        input = batch[batch_value][1]\n",
    "        input = T.complex_abs(input)\n",
    "        input = T.center_crop(input, (320, 320))\n",
    "        data[batch_value, 0, :, :] = input\n",
    "        \n",
    "        target = batch[batch_value][0]\n",
    "        target = T.complex_abs(target)\n",
    "        target = T.center_crop(target, (320, 320))\n",
    "        target_list[batch_value, 0, :, :] = target\n",
    "    return (target_list, data, None, None, None)\n",
    "    \n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=5, num_workers=num_workers, collate_fn=my_collate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    2 lots of:\n",
    "        3x3 convolution\n",
    "        batch normalisation\n",
    "        ReLu  \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, padding_mode='zeros', bias=False),\n",
    "            nn.BatchNorm2d(out_chans),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        #print('input', input.shape)\n",
    "        return self.layers(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Unet model\n",
    "        \n",
    "        Encoder:\n",
    "            input -> Convolutional Block (output 1)\n",
    "            \n",
    "            maxpool(output 1) -> Convolutional Block (output 2)\n",
    "            maxpool(output 2) -> Convolutional Block (output 3)\n",
    "                        \n",
    "            concat(output 2, deconv(output 3)) -> Convolutional Block (output 4)\n",
    "            \n",
    "            concat(output 1, deconv(output 4)) -> Convolutional Block (output 5)\n",
    "            \n",
    "            output5 -> 1x1 Convolution (output 6)\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_chans, output_chans, chans, depth=3, level_len=1, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.chans = chans\n",
    "        self.depth = depth\n",
    "        self.level_len = level_len\n",
    "        self.encoding_layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        out_chans = chans\n",
    "        \n",
    "        # First level\n",
    "        self.encoding_layers.append(ConvolutionalBlock(input_chans, chans))\n",
    "        \n",
    "        for j in range(level_len):\n",
    "            self.encoding_layers.append(ConvolutionalBlock(chans, chans))\n",
    "            \n",
    "        self.encoding_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        '''ENCODER'''\n",
    "        for i in range(depth-2):\n",
    "            in_chans = out_chans\n",
    "            out_chans = int(out_chans * 2)\n",
    "            self.encoding_layers.append(ConvolutionalBlock(in_chans, out_chans))\n",
    "            \n",
    "            for j in range(level_len):\n",
    "                self.encoding_layers.append(ConvolutionalBlock(out_chans, out_chans))\n",
    "                \n",
    "            self.encoding_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            #self.encoding_layers.append(self.dropout)\n",
    "            \n",
    "                               \n",
    "        in_chans = out_chans\n",
    "        out_chans = int(out_chans * 2)\n",
    "        self.encoding_layers.append(ConvolutionalBlock(in_chans, out_chans))\n",
    "        \n",
    "        for j in range(level_len):\n",
    "            self.encoding_layers.append(ConvolutionalBlock(out_chans, out_chans))       \n",
    "        \n",
    "        \n",
    "        \n",
    "        '''DECODER'''\n",
    "        self.decoding_layers = nn.ModuleList()\n",
    "                    \n",
    "                                    \n",
    "        in_chans = out_chans\n",
    "        out_chans = int(in_chans / 2)\n",
    "                                    \n",
    "        self.decoding_layers.append(nn.ConvTranspose2d(in_chans, out_chans, kernel_size=2, stride=2)) \n",
    "                                    \n",
    "        for i in range(depth-2):\n",
    "            \n",
    "            self.decoding_layers.append(ConvolutionalBlock(in_chans, out_chans))\n",
    "            \n",
    "            for j in range(level_len):\n",
    "                self.decoding_layers.append(ConvolutionalBlock(out_chans, out_chans))\n",
    "                \n",
    "            in_chans = out_chans\n",
    "            out_chans = int(out_chans / 2)\n",
    "            self.decoding_layers.append(nn.ConvTranspose2d(in_chans, out_chans, kernel_size=2, stride=2))                        \n",
    "            #self.decoding_layers.append(self.dropout)\n",
    "                                    \n",
    "        self.decoding_layers.append(ConvolutionalBlock(in_chans, out_chans))\n",
    "        \n",
    "        for j in range(level_len): \n",
    "            self.decoding_layers.append(ConvolutionalBlock(out_chans, out_chans))\n",
    "        \n",
    "        '''FINAL CONV'''\n",
    "        \n",
    "        self.finalConv = nn.Conv2d(in_channels=out_chans, out_channels=output_chans, kernel_size=1)\n",
    "        \n",
    "       \n",
    "        ''''''\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        level_output = []\n",
    "        index_to_save = 1\n",
    "                                    \n",
    "        '''ENCODE'''  \n",
    "        \n",
    "        for i, layer in enumerate(self.encoding_layers):\n",
    "            input = layer(input)\n",
    "            if i == index_to_save: \n",
    "                level_output.append(input)\n",
    "                index_to_save += 2 + self.level_len             \n",
    "                                    \n",
    "                                    \n",
    "\n",
    "        '''DECODE'''\n",
    "        index_to_cat = 0\n",
    "        for i, layer in enumerate(self.decoding_layers):\n",
    "            input = layer(input)\n",
    "            if i == index_to_cat:\n",
    "                level_output.pop()\n",
    "                index_to_cat += 2 + self.level_len  \n",
    "                input =  torch.cat((input, level_output[-1]), dim=1)\n",
    "\n",
    "        \n",
    "        output = self.finalConv(input)\n",
    "\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7a18152550>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a number of models for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer length 2, depth 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer length 2, depth 5\n",
    "model_l2_d5 = UNet(\n",
    "    input_chans=1,\n",
    "    output_chans=1,\n",
    "    chans=32,\n",
    "    depth=5,\n",
    "    level_len=2,\n",
    "    drop_prob=0.5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer length 2, depth 7\n",
    "model_l2_d7 = UNet(\n",
    "    input_chans=1,\n",
    "    output_chans=1,\n",
    "    chans=32,\n",
    "    depth=7,\n",
    "    level_len=2,\n",
    "    drop_prob=0.5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.25 GiB (GPU 0; 5.77 GiB total capacity; 3.75 GiB already allocated; 1.17 GiB free; 3.88 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4f981736a6fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlevel_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdrop_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m ).to(device)\n\u001b[0m",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/bham/modules/roots/neural-comp/2019-20/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.25 GiB (GPU 0; 5.77 GiB total capacity; 3.75 GiB already allocated; 1.17 GiB free; 3.88 MiB cached)"
     ]
    }
   ],
   "source": [
    "# layer length 2, depth 9\n",
    "model_l2_d9 = UNet(\n",
    "    input_chans=1,\n",
    "    output_chans=1,\n",
    "    chans=32,\n",
    "    depth=9,\n",
    "    level_len=2,\n",
    "    drop_prob=0.5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer length 1, depth 5\n",
    "model_l1_d5 = UNet(\n",
    "    input_chans=1,\n",
    "    output_chans=1,\n",
    "    chans=32,\n",
    "    depth=5,\n",
    "    level_len=1,\n",
    "    drop_prob=0.5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssim loss\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "\n",
    "def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):\n",
    "    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
    "    if val_range is None:\n",
    "        if torch.max(img1) > 128:\n",
    "            max_val = 255\n",
    "        else:\n",
    "            max_val = 1\n",
    "\n",
    "        if torch.min(img1) < -0.5:\n",
    "            min_val = -1\n",
    "        else:\n",
    "            min_val = 0\n",
    "        L = max_val - min_val\n",
    "    else:\n",
    "        L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Classes to re-use window\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True, val_range=None):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "\n",
    "        # Assume 1 channel for SSIM\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "        \n",
    "        if channel == self.channel and self.window.type == img1.type:\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel).to(img1.device).type(img1.dtype)\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "        return 1 - ssim(img2, img1, window=window, window_size=self.window_size, size_average=self.size_average, val_range=img2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a3ec5d88e263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#stochastic gradient descent (SGD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# optimiser = optim.SGD(model.parameters(), lr=lr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptimiser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# optimiser = torch.optim.RMSprop(model.parameters(), lr, weight_decay=wd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# set learning rate\n",
    "lr = 1e-4\n",
    "wd = 0.0\n",
    "#optimiser\n",
    "#stochastic gradient descent (SGD)\n",
    "# optimiser = optim.SGD(model.parameters(), lr=lr)\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimiser = torch.optim.RMSprop(model.parameters(), lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_loss = SSIM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, loss):\n",
    "    full_path = path + \"-loss-\" + str(loss) + '.h5'\n",
    "    torch.save(model.state_dict(), full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name):\n",
    "    optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "    file_name = 'loss_log_' + model_name + '.csv'\n",
    "    file = open(file_name, 'a', newline='')\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Epoch\", \"Loss\"])\n",
    "    #train the network \n",
    "\n",
    "    # set number of epoches, i.e., number of times we iterate through the training set\n",
    "    epoches = 30\n",
    "\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        model.train() \n",
    "        mean = []\n",
    "        for iter, data in enumerate(train_loader):\n",
    "            target, input, _, _, _ = data\n",
    "            # print(input.shape)\n",
    "            input = input.to(device)\n",
    "            # print(input.shape)\n",
    "            # print(target.shape)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(input)\n",
    "            # print(output.shape)\n",
    "            loss = ssim_loss(output, target)\n",
    "            # loss = msssim_loss(output, target)\n",
    "            # loss = F.l1_loss(output, target)\n",
    "            mean.append(loss)\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        l = sum(mean)/len(mean)\n",
    "        print(\"Epoch {}'s loss: {}\".format(epoch, l))\n",
    "        writer.writerow([\"{}\".format(epoch), \"{}\".format(l)])\n",
    "\n",
    "        #if(epoch % 50 == 0):\n",
    "            #save_model('models/model', str(l))\n",
    "\n",
    "        # print(\"After training: \\n\", model.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'model_final.h5'\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#model.load_state_dict(torch.load('model_final.h5'))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader for training set. It applies same to validation set as well\n",
    "val_dataset = MRIDataset(data_list['val'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=14, num_workers=num_workers, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim \n",
    "def ssim_old(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = open('model_accuracy.csv', 'a', newline='')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow([\"Model\", \"Mean Score\"])\n",
    "    \n",
    "def evaluate_model_accuracy(model, model_name):\n",
    "    ssim_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for iteration, sample in enumerate(val_loader):\n",
    "\n",
    "            img_gt, img_und, _, _, _  = sample\n",
    "            D = img_gt\n",
    "\n",
    "            C = img_und\n",
    "\n",
    "            output = model(img_und.to(device)).cpu().numpy().squeeze()\n",
    "            ssim_scores.append(ssim_old(D.squeeze(1).numpy(), output))\n",
    "\n",
    "    numpy_ssims = np.array(ssim_scores)\n",
    "    print(\"len of list\", len(ssim_scores))\n",
    "    print(\"Mean:\", numpy_ssims.mean())\n",
    "    writer.writerow([model_name, numpy_ssims.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and evaluate all the test models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_l5_d5, 'l1_d5')\n",
    "evaluate_model_accuracy(model_l5_d5, 'l1_d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer length 4, depth 5\n",
    "model_l4_d5 = UNet(\n",
    "    input_chans=1,\n",
    "    output_chans=1,\n",
    "    chans=32,\n",
    "    depth=5,\n",
    "    level_len=4,\n",
    "    drop_prob=0.5\n",
    ").to(device)\n",
    "evaluate_model_accuracy(model_l4_d5, 'l4_d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer length 3, depth 5\n",
    "model_l3_d5 = UNet(\n",
    "    input_chans=1,\n",
    "    output_chans=1,\n",
    "    chans=32,\n",
    "    depth=5,\n",
    "    level_len=3,\n",
    "    drop_prob=0.5\n",
    ").to(device)\n",
    "train_model(model_l3_d5, 'l3_d5')\n",
    "evaluate_model_accuracy(model_l3_d5, 'l3_d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_l1_d5, 'l1_d5')\n",
    "evaluate_model_accuracy(model_l1_d5, 'l1_d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_l2_d9, 'l2_d9')\n",
    "evaluate_model_accuracy(model_l2_d9, 'l2_d9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_l2_d7, 'l2_d7')\n",
    "evaluate_model_accuracy(model_l2_d7, 'l2_d7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_l2_d5, 'l2_d5')\n",
    "evaluate_model_accuracy(model_l2_d5, 'l2_d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0's loss: 0.5896424055099487\n",
      "Epoch 1's loss: 0.4461091160774231\n",
      "Epoch 2's loss: 0.44216984510421753\n",
      "Epoch 3's loss: 0.42357707023620605\n",
      "Epoch 4's loss: 0.4200793504714966\n",
      "Epoch 5's loss: 0.4198910892009735\n",
      "Epoch 6's loss: 0.41208645701408386\n",
      "Epoch 7's loss: 0.4190998077392578\n",
      "Epoch 8's loss: 0.40705007314682007\n",
      "Epoch 9's loss: 0.3955399990081787\n",
      "Epoch 10's loss: 0.4109104573726654\n",
      "Epoch 11's loss: 0.40756818652153015\n",
      "Epoch 12's loss: 0.41081202030181885\n",
      "Epoch 13's loss: 0.4076380133628845\n",
      "Epoch 14's loss: 0.40458524227142334\n",
      "Epoch 15's loss: 0.40374207496643066\n",
      "Epoch 16's loss: 0.3984769284725189\n",
      "Epoch 17's loss: 0.402484655380249\n",
      "Epoch 18's loss: 0.3934868574142456\n",
      "Epoch 19's loss: 0.4004673659801483\n",
      "Epoch 20's loss: 0.400204062461853\n",
      "Epoch 21's loss: 0.39846035838127136\n",
      "Epoch 22's loss: 0.3943627178668976\n",
      "Epoch 23's loss: 0.3960895240306854\n",
      "Epoch 24's loss: 0.39752835035324097\n",
      "Epoch 25's loss: 0.39632388949394226\n",
      "Epoch 26's loss: 0.3966689705848694\n",
      "Epoch 27's loss: 0.3902733623981476\n",
      "Epoch 28's loss: 0.38276225328445435\n",
      "Epoch 29's loss: 0.39233899116516113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of list 27\n",
      "Mean: 0.6072865442860597\n"
     ]
    }
   ],
   "source": [
    "model_l2_d3 = UNet(\n",
    "    input_chans=1,\n",
    "    output_chans=1,\n",
    "    chans=32,\n",
    "    depth=3,\n",
    "    level_len=2,\n",
    "    drop_prob=0.5\n",
    ").to(device)\n",
    "train_model(model_l2_d3, 'l2_d3')\n",
    "evaluate_model_accuracy(model_l2_d3, 'l2_d3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iteration, sample in enumerate(val_loader):\n",
    "    \n",
    "        img_gt, img_und, _, _, _  = sample\n",
    "        D = img_gt\n",
    "        \n",
    "        C = img_und\n",
    "    \n",
    "        output = model(img_und.to(device)).cpu().numpy().squeeze()\n",
    "        ssim_scores.append(ssim_old(D.squeeze(1).numpy(), output))\n",
    "            \n",
    "numpy_ssims = np.array(ssim_scores)\n",
    "print(\"len of list\", len(ssim_scores))\n",
    "print(\"Mean:\", numpy_ssims.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for iteration, sample in enumerate(val_loader):\n",
    "    \n",
    "        img_gt, img_und, _, _, _  = sample\n",
    "        D = img_gt\n",
    "        \n",
    "        C = img_und\n",
    "    \n",
    "        output = model(img_und.to(device)).cpu().numpy().squeeze()\n",
    "        show_slices([D.squeeze(1).numpy()[6], output[6]], [0, 1], cmap='gray')\n",
    "        \n",
    "        if iteration >= 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructions(reconstructions, out_dir, filename):\n",
    "    \"\"\"\n",
    "    Saves the reconstructions from a model into h5 files that is appropriate for submission\n",
    "    to the leaderboard.\n",
    "    Args:\n",
    "        reconstructions (dict[str, np.array]): A dictionary mapping input filenames to\n",
    "            corresponding reconstructions (of shape num_slices x height x width).\n",
    "        out_dir (pathlib.Path): Path to the output directory where the reconstructions\n",
    "            should be saved.\n",
    "    \"\"\"\n",
    "    # print(len(reconstructions))\n",
    "    for fname, recons in reconstructions.items():\n",
    "        subject_path = os.path.join(out_dir, filename)\n",
    "        print(subject_path)\n",
    "        with h5py.File(subject_path, 'a') as f:\n",
    "            f.create_dataset(fname, data=recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/data/local/NC2019MRI/test/'\n",
    "files = []\n",
    "file_names = []\n",
    "\n",
    "for r, d, f in os.walk(test_path):\n",
    "    for file in f:\n",
    "        files.append(os.path.join(r, file))\n",
    "        file_names.append(file)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(files)):\n",
    "        with h5py.File(files[i],  \"r\") as hf:\n",
    "            volume_kspace_4af = hf['kspace_4af'][()]\n",
    "            volume_kspace_8af = hf['kspace_8af'][()]\n",
    "            volume_kspace4 = T.to_tensor(volume_kspace_4af)\n",
    "            volume_kspace8 = T.to_tensor(volume_kspace_8af) \n",
    "            _4af, _8af = T.ifft2(volume_kspace4), T.ifft2(volume_kspace8)\n",
    "            norm_4af = T.complex_abs(_4af).max()\n",
    "            if norm_4af < 1e-6: norm_4af = 1e-6\n",
    "            norm_8af = T.complex_abs(_8af).max()\n",
    "            if norm_8af < 1e-6: norm_8af = 1e-6\n",
    "            _4af = _4af / norm_4af\n",
    "            _8af = _8af / norm_8af\n",
    "            _4af = T.complex_abs(_4af.squeeze(0))\n",
    "            _4af = T.center_crop(_4af, (320, 320)).to(device)\n",
    "            _8af = T.complex_abs(_8af.squeeze(0))\n",
    "            _8af = T.center_crop(_8af, (320, 320)).to(device)\n",
    "\n",
    "            recon_4af = model(_4af.unsqueeze(1)).squeeze(1).cpu()\n",
    "            recon_8af = model(_8af.unsqueeze(1)).squeeze(1).cpu()\n",
    "            # print(recon_4af.shape)\n",
    "            reconstructions = {'recon_4af': recon_4af.numpy(), 'recon_8af': recon_8af.numpy()}\n",
    "            out_dir = 'saved/' # where you want to save your result. \n",
    "            if not (os.path.exists(out_dir)): os.makedirs(out_dir)\n",
    "            save_reconstructions(reconstructions, out_dir, file_names[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affile_path = 'saved/file1000817.h5'\n",
    "\n",
    "with h5py.File(file_path,  \"r\") as hf:\n",
    "    img1 = hf['recon_4af']\n",
    "    img2 = hf['recon_8af']\n",
    "    print(img2.shape)\n",
    "    \n",
    "    show_slices([img1[20],img2[20]], [0, 1], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
