{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "import h5py, os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from functions import transforms as T\n",
    "from torch.nn import functional as F\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  # check whether a GPU is available\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=center_fract, accelerations=acc)\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und = img_gt/norm, img_und/norm\n",
    "        \n",
    "    img_gt, img_und = img_gt.squeeze(0), img_und.squeeze(0)\n",
    "    \n",
    "    img_gt, img_und = T.complex_abs(img_gt), T.complex_abs(img_und)\n",
    "    \n",
    "    return T.center_crop(img_gt, (320, 320)), T.center_crop(img_und, (320, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    limit = 60\n",
    "    \n",
    "        \n",
    "    l = sorted(os.listdir(data_path))\n",
    "    \n",
    "    for i in range(len(train_and_val)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        if i == 0 : la = l[:limit]\n",
    "        else : la = l[limit:]\n",
    "    \n",
    "        for fname in la:\n",
    "                \n",
    "            subject_data_path = os.path.join(data_path, fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue\n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            if i == 1:\n",
    "                data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(0, num_slice)]\n",
    "            else:\n",
    "                data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(14, 25)]\n",
    "    \n",
    "    return data_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "#PREPARE THE DATA \n",
    "data_list = load_data_path('/data/local/NC2019MRI/train')\n",
    "# slices, height, width = input_k.shape()\n",
    "\n",
    "acc = [4,8]\n",
    "cen_fract = [0.08, 0.04]\n",
    "seed = False # random masks for each slice \n",
    "num_workers = 10 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=5, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    2 lots of:\n",
    "        3x3 convolution\n",
    "        batch normalisation\n",
    "        ReLu  \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, padding_mode='zeros', bias=False),\n",
    "            nn.BatchNorm2d(out_chans),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        #print('input', input.shape)\n",
    "        return self.layers(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Unet model\n",
    "        \n",
    "        Encoder:\n",
    "            input -> Convolutional Block (output 1)\n",
    "            \n",
    "            maxpool(output 1) -> Convolutional Block (output 2)\n",
    "            maxpool(output 2) -> Convolutional Block (output 3)\n",
    "                        \n",
    "            concat(output 2, deconv(output 3)) -> Convolutional Block (output 4)\n",
    "            \n",
    "            concat(output 1, deconv(output 4)) -> Convolutional Block (output 5)\n",
    "            \n",
    "            output5 -> 1x1 Convolution (output 6)\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_chans, output_chans, chans, depth=3, level_len=1, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.chans = chans\n",
    "        self.depth = depth\n",
    "        self.level_len = level_len\n",
    "        self.encoding_layers = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        out_chans = chans\n",
    "        \n",
    "        # First level\n",
    "        self.encoding_layers.append(ConvolutionalBlock(input_chans, chans))\n",
    "        \n",
    "        for j in range(level_len):\n",
    "            self.encoding_layers.append(ConvolutionalBlock(chans, chans))\n",
    "            \n",
    "        self.encoding_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        '''ENCODER'''\n",
    "        for i in range(depth-2):\n",
    "            in_chans = out_chans\n",
    "            out_chans = int(out_chans * 2)\n",
    "            self.encoding_layers.append(ConvolutionalBlock(in_chans, out_chans))\n",
    "            \n",
    "            for j in range(level_len):\n",
    "                self.encoding_layers.append(ConvolutionalBlock(out_chans, out_chans))\n",
    "                \n",
    "            self.encoding_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            #self.encoding_layers.append(self.dropout)\n",
    "            \n",
    "                               \n",
    "        in_chans = out_chans\n",
    "        out_chans = int(out_chans * 2)\n",
    "        self.encoding_layers.append(ConvolutionalBlock(in_chans, out_chans))\n",
    "        \n",
    "        for j in range(level_len):\n",
    "            self.encoding_layers.append(ConvolutionalBlock(out_chans, out_chans))       \n",
    "        \n",
    "        \n",
    "        \n",
    "        '''DECODER'''\n",
    "        self.decoding_layers = nn.ModuleList()\n",
    "                    \n",
    "                                    \n",
    "        in_chans = out_chans\n",
    "        out_chans = int(in_chans / 2)\n",
    "                                    \n",
    "        self.decoding_layers.append(nn.ConvTranspose2d(in_chans, out_chans, kernel_size=2, stride=2)) \n",
    "                                    \n",
    "        for i in range(depth-2):\n",
    "            \n",
    "            self.decoding_layers.append(ConvolutionalBlock(in_chans, out_chans))\n",
    "            \n",
    "            for j in range(level_len):\n",
    "                self.decoding_layers.append(ConvolutionalBlock(out_chans, out_chans))\n",
    "                \n",
    "            in_chans = out_chans\n",
    "            out_chans = int(out_chans / 2)\n",
    "            self.decoding_layers.append(nn.ConvTranspose2d(in_chans, out_chans, kernel_size=2, stride=2))                        \n",
    "            #self.decoding_layers.append(self.dropout)\n",
    "                                    \n",
    "        self.decoding_layers.append(ConvolutionalBlock(in_chans, out_chans))\n",
    "        \n",
    "        for j in range(level_len): \n",
    "            self.decoding_layers.append(ConvolutionalBlock(out_chans, out_chans))\n",
    "        \n",
    "        '''FINAL CONV'''\n",
    "        \n",
    "        self.finalConv = nn.Conv2d(in_channels=out_chans, out_channels=output_chans, kernel_size=1)\n",
    "        \n",
    "       \n",
    "        ''''''\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        level_output = []\n",
    "        index_to_save = 1\n",
    "                                    \n",
    "        '''ENCODE'''  \n",
    "        \n",
    "        for i, layer in enumerate(self.encoding_layers):\n",
    "            input = layer(input)\n",
    "            if i == index_to_save: \n",
    "                level_output.append(input)\n",
    "                index_to_save += 2 + self.level_len             \n",
    "                                    \n",
    "                                    \n",
    "\n",
    "        '''DECODE'''\n",
    "        index_to_cat = 0\n",
    "        for i, layer in enumerate(self.decoding_layers):\n",
    "            input = layer(input)\n",
    "            if i == index_to_cat:\n",
    "                level_output.pop()\n",
    "                index_to_cat += 2 + self.level_len  \n",
    "                input =  torch.cat((input, level_output[-1]), dim=1)\n",
    "\n",
    "        \n",
    "        output = self.finalConv(input)\n",
    "\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#create a model\n",
    "model = UNet(\n",
    "    input_chans=1,\n",
    "    output_chans=1,\n",
    "    chans=32,\n",
    "    depth=5,\n",
    "    level_len=2,\n",
    "    drop_prob=0.5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssim loss\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "\n",
    "def ssim(img1, img2, window_size=11, window=None, val_range=img1.max()):\n",
    "    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    return ret = ssim_map.mean()\n",
    "\n",
    "\n",
    "\n",
    "# Classes to re-use window\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size=11):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Assume 1 channel for SSIM\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size)\n",
    "\n",
    "    def forward(self, output_img, target_img):\n",
    "        (_, channel, _, _) = output_img.size()\n",
    "        \n",
    "        if channel == self.channel and self.window.type == output_img.type:\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel).to(output_img.device).type(output_img.dtype)\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "        return 1 - ssim(target_img, output_img, window=window, window_size=self.window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "lr = 1e-4\n",
    "wd = 0.0\n",
    "#optimiser\n",
    "#stochastic gradient descent (SGD)\n",
    "# optimiser = optim.SGD(model.parameters(), lr=lr)\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimiser = torch.optim.RMSprop(model.parameters(), lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_loss = SSIM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, loss):\n",
    "    full_path = path + \"-loss-\" + str(loss) + '.h5'\n",
    "    torch.save(model.state_dict(), full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv \n",
    "file = open('loss_log.csv', 'w', newline='')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow([\"Epoch\", \"Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0's loss: 0.5326647162437439\n",
      "Epoch 1's loss: 0.42771729826927185\n",
      "Epoch 2's loss: 0.4201389253139496\n",
      "Epoch 3's loss: 0.408461332321167\n",
      "Epoch 4's loss: 0.4108220934867859\n",
      "Epoch 5's loss: 0.40335795283317566\n",
      "Epoch 6's loss: 0.3992147743701935\n",
      "Epoch 7's loss: 0.40541133284568787\n",
      "Epoch 8's loss: 0.40148040652275085\n",
      "Epoch 9's loss: 0.3980918824672699\n",
      "Epoch 10's loss: 0.39697209000587463\n",
      "Epoch 11's loss: 0.3911105692386627\n",
      "Epoch 12's loss: 0.39501485228538513\n",
      "Epoch 13's loss: 0.38961970806121826\n",
      "Epoch 14's loss: 0.3884909152984619\n",
      "Epoch 15's loss: 0.38434305787086487\n",
      "Epoch 16's loss: 0.39106836915016174\n",
      "Epoch 17's loss: 0.3962933421134949\n",
      "Epoch 18's loss: 0.38582029938697815\n",
      "Epoch 19's loss: 0.3879324793815613\n",
      "Epoch 20's loss: 0.38138246536254883\n",
      "Epoch 21's loss: 0.3769466280937195\n",
      "Epoch 22's loss: 0.38027673959732056\n",
      "Epoch 23's loss: 0.38662102818489075\n",
      "Epoch 24's loss: 0.3829851746559143\n",
      "Epoch 25's loss: 0.3695758581161499\n",
      "Epoch 26's loss: 0.37842392921447754\n"
     ]
    }
   ],
   "source": [
    "#train the network \n",
    "\n",
    "# set number of epoches, i.e., number of times we iterate through the training set\n",
    "epoches = 50\n",
    "\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    model.train() \n",
    "    mean = []\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        target_img, input_img = data\n",
    "        input_img = input_img.to(device)\n",
    "        target_img = target_img.to(device)\n",
    "\n",
    "        output_img = model(input_img)\n",
    "        \n",
    "        loss = ssim_loss(output_img, target_img)\n",
    "        # loss = msssim_loss(output, target)\n",
    "        # loss = F.l1_loss(output, target)\n",
    "        mean.append(loss)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    l = sum(mean)/len(mean)\n",
    "    print(\"Epoch {}'s loss: {}\".format(epoch, l))\n",
    "    writer.writerow([\"{}\".format(epoch), \"{}\".format(l)])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'model_latest.h5'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#model.load_state_dict(torch.load('model_final.h5'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader for training set. It applies same to validation set as well\n",
    "val_dataset = MRIDataset(data_list['val'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=38, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim \n",
    "def ssim_numpy(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iteration, sample in enumerate(val_loader):\n",
    "    \n",
    "        img_gt, img_und = sample\n",
    "    \n",
    "        output_img = model(img_und.to(device)).cpu().numpy().squeeze()\n",
    "        ssim_scores.append(ssim_numpy(img_gt.squeeze(1).numpy(), output_img))\n",
    "            \n",
    "numpy_ssims = np.array(ssim_scores)\n",
    "print(\"len of list\", len(ssim_scores))\n",
    "print(\"Mean:\", numpy_ssims.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for iteration, sample in enumerate(val_loader):\n",
    "    \n",
    "        img_gt, img_und = sample\n",
    "    \n",
    "        output_img = model(img_und.to(device)).cpu().numpy().squeeze()\n",
    "        show_slices([img_gt.squeeze(1).numpy()[6], output_img[6]], [0, 1], cmap='gray')\n",
    "        \n",
    "        if iteration >= 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstructions(reconstructions, out_dir, file_name):\n",
    "\n",
    "    for fname, recons in reconstructions.items():\n",
    "        subject_path = os.path.join(out_dir, file_name)\n",
    "        with h5py.File(subject_path, 'a') as f:\n",
    "            f.create_dataset(fname, data=recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/data/local/NC2019MRI/test/'\n",
    "files = []\n",
    "file_names = []\n",
    "\n",
    "for r, d, f in os.walk(test_path):\n",
    "    for file in f:\n",
    "        files.append(os.path.join(r, file))\n",
    "        file_names.append(file)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(files)):\n",
    "        with h5py.File(files[i],  \"r\") as hf:\n",
    "            volume_kspace_4af = hf['kspace_4af'][()]\n",
    "            volume_kspace_8af = hf['kspace_8af'][()]\n",
    "            volume_kspace4 = T.to_tensor(volume_kspace_4af)\n",
    "            volume_kspace8 = T.to_tensor(volume_kspace_8af) \n",
    "            _4af, _8af = T.ifft2(volume_kspace4), T.ifft2(volume_kspace8)\n",
    "            norm_4af = T.complex_abs(_4af).max()\n",
    "            if norm_4af < 1e-6: norm_4af = 1e-6\n",
    "            norm_8af = T.complex_abs(_8af).max()\n",
    "            if norm_8af < 1e-6: norm_8af = 1e-6\n",
    "            _4af = _4af / norm_4af\n",
    "            _8af = _8af / norm_8af\n",
    "            _4af = T.complex_abs(_4af.squeeze(0))\n",
    "            _4af = T.center_crop(_4af, (320, 320)).to(device)\n",
    "            _8af = T.complex_abs(_8af.squeeze(0))\n",
    "            _8af = T.center_crop(_8af, (320, 320)).to(device)\n",
    "\n",
    "            recon_4af = model(_4af.unsqueeze(1)).squeeze(1).cpu()\n",
    "            recon_8af = model(_8af.unsqueeze(1)).squeeze(1).cpu()\n",
    "\n",
    "            reconstructions = {'recon_4af': recon_4af.numpy(), 'recon_8af': recon_8af.numpy()}\n",
    "            out_dir = 'saved/'\n",
    "            if not (os.path.exists(out_dir)): os.makedirs(out_dir)\n",
    "            save_reconstructions(reconstructions, out_dir, file_names[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affile_path = 'saved/file1000817.h5'\n",
    "\n",
    "with h5py.File(file_path,  \"r\") as hf:\n",
    "    img1 = hf['recon_4af']\n",
    "    img2 = hf['recon_8af']\n",
    "    print(img2.shape)\n",
    "    \n",
    "    show_slices([img1[20],img2[20]], [0, 1], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
