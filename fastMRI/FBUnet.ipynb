{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "import h5py, os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from functions import transforms as T\n",
    "from torch.nn import functional as F\n",
    "from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from math import exp\n",
    "import torch.optim as optim\n",
    "from skimage.measure import compare_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'  # check whether a GPU is available\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=False):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "    with h5py.File(rawdata_name, 'r') as data:\n",
    "        rawdata = data['kspace'][slice]\n",
    "                      \n",
    "    slice_kspace = T.to_tensor(rawdata).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=center_fract, accelerations=acc)\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "\n",
    "    img_gt, img_und = T.ifft2(slice_kspace), T.ifft2(masked_kspace)\n",
    "\n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = T.complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    # normalized data\n",
    "    img_gt, img_und = img_gt/norm, img_und/norm\n",
    "        \n",
    "    img_gt, img_und = img_gt.squeeze(0), img_und.squeeze(0)\n",
    "    \n",
    "    img_gt, img_und = T.complex_abs(img_gt), T.complex_abs(img_und)\n",
    "    \n",
    "    return T.center_crop(img_gt, (320, 320)), T.center_crop(img_und, (320, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    limit = 60\n",
    "    \n",
    "        \n",
    "    l = sorted(os.listdir(data_path))\n",
    "    \n",
    "    for i in range(len(train_and_val)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        if i == 0 : la = l[:limit]\n",
    "        else : la = l[limit:]\n",
    "    \n",
    "        for fname in la:\n",
    "                \n",
    "            subject_data_path = os.path.join(data_path, fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue\n",
    "            \n",
    "            with h5py.File(subject_data_path, 'r') as data:\n",
    "                num_slice = data['kspace'].shape[0]\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            if i == 1:\n",
    "                data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(0, num_slice)]\n",
    "            else:\n",
    "                data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(14, 25)]\n",
    "    \n",
    "    return data_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARE THE DATA \n",
    "data_list = load_data_path('/data/local/NC2019MRI/train')\n",
    "\n",
    "acc = [4,8]\n",
    "cen_fract = [0.08, 0.04]\n",
    "num_workers = 10 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "# create data loader for training set. It applies same to validation set as well\n",
    "train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=14, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    2 lots of:\n",
    "        3x3 convolutional blocks\n",
    "        Instance Normalisation \n",
    "        ReLu\n",
    "        Dropout \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, drop_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_chans),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(drop_prob)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        #print('input', input.shape)\n",
    "        return self.layers(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Unet model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans, out_chans, chans, num_pool_layers, drop_prob):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans (int): Number of channels in the input to the U-Net model.\n",
    "            out_chans (int): Number of channels in the output to the U-Net model.\n",
    "            chans (int): Number of output channels of the first convolution layer.\n",
    "            num_pool_layers (int): Number of down-sampling and up-sampling layers.\n",
    "            drop_prob (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.chans = chans\n",
    "        self.num_pool_layers = num_pool_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.layers_list_downsample = nn.ModuleList()\n",
    "        convblock_1 = ConvolutionalBlock(in_chans, chans, drop_prob)\n",
    "        self.layers_list_downsample += [convblock_1]\n",
    "        \n",
    "        #print(in_chans, chans)\n",
    "        \n",
    "        ch = chans\n",
    "        #create a new convolutionalm block for each layer, doubling the number of channels to downsample \n",
    "        for i in range(num_pool_layers - 1):\n",
    "            new_convBlock = ConvolutionalBlock(ch, ch * 2, drop_prob)\n",
    "            self.layers_list_downsample += [new_convBlock]\n",
    "            #print(ch, ch*2)\n",
    "            ch *= 2\n",
    "        #one for convolution block with the same number of channels as the previous    \n",
    "        self.conv = ConvolutionalBlock(ch, ch, drop_prob)\n",
    "        #print(ch, ch)\n",
    "\n",
    "        #the same thing but decreasing the chanells to upsample \n",
    "        self.layers_list_upsample = nn.ModuleList()\n",
    "        for i in range(num_pool_layers - 1):\n",
    "            new_convBlock = ConvolutionalBlock(ch * 2, ch // 2, drop_prob)\n",
    "            self.layers_list_upsample += [new_convBlock]\n",
    "            #print(ch * 2, ch // 2)\n",
    "            ch //= 2\n",
    "        self.layers_list_upsample += [ConvolutionalBlock(ch * 2, ch, drop_prob)]\n",
    "        #print(ch*2, ch)\n",
    "        \n",
    "        #2 convolution layers to build the data up to the same size at the input \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch // 2, kernel_size=1),\n",
    "            nn.Conv2d(ch // 2, out_chans, kernel_size=1),\n",
    "            nn.Conv2d(out_chans, out_chans, kernel_size=1),\n",
    "        )\n",
    "        #print(ch, ch // 2 )\n",
    "        #print(ch // 2, out_chans)\n",
    "        #print(out_chans, out_chans)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape [batch_size, self.in_chans, height, width]\n",
    "        Returns:\n",
    "            (torch.Tensor): Output tensor of shape [batch_size, self.out_chans, height, width]\n",
    "        \"\"\"\n",
    "        stack = []\n",
    "        output = input\n",
    "        # Apply down-sampling layers\n",
    "        for layer in self.layers_list_downsample:\n",
    "            output = layer(output)\n",
    "            #print('output', output.shape)\n",
    "            stack.append(output)\n",
    "            #apply 2x2 max pooling \n",
    "            output = F.max_pool2d(output, kernel_size=2)\n",
    "\n",
    "        output = self.conv(output)\n",
    "        #print('output', output.shape)\n",
    "        \n",
    "        #print('up-sampling')\n",
    "\n",
    "        # Apply up-sampling layers\n",
    "        for layer in self.layers_list_upsample:\n",
    "            #print(output.shape, stack[-1].shape)\n",
    "            output = F.interpolate(output, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "            #print(output.shape, stack[-1].shape)\n",
    "            output = torch.cat([output, stack.pop()], dim=1)\n",
    "            output = layer(output)\n",
    "        return self.conv2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#create a model\n",
    "model = NeuralNetworkModel(\n",
    "    in_chans=1,\n",
    "    out_chans=1,\n",
    "    chans=32,\n",
    "    num_pool_layers=4,\n",
    "    drop_prob=0.0\n",
    ").to(device)\n",
    "\n",
    "#inspect parameters \n",
    "# print(\"Before training: \\n\", model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "#start point: L1 loss |output - gold standard|\n",
    "\n",
    "#ssim loss\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "\n",
    "def ssim(img1, img2, window_size=11, window=None, size_average=True, full=False, val_range=None):\n",
    "    # Value range can be different from 255. Other common ranges are 1 (sigmoid) and 2 (tanh).\n",
    "    if val_range is None:\n",
    "        if torch.max(img1) > 128:\n",
    "            max_val = 255\n",
    "        else:\n",
    "            max_val = 1\n",
    "\n",
    "        if torch.min(img1) < -0.5:\n",
    "            min_val = -1\n",
    "        else:\n",
    "            min_val = 0\n",
    "        L = max_val - min_val\n",
    "    else:\n",
    "        L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Classes to re-use window\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True, val_range=None):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "\n",
    "        # Assume 1 channel for SSIM\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "        \n",
    "        if channel == self.channel and self.window.type == img1.type:\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel).to(img1.device).type(img1.dtype)\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "        return 1 - ssim(img2, img1, window=window, window_size=self.window_size, size_average=self.size_average, val_range=img2.max())\n",
    "        \n",
    "def msssim(img1, img2, window_size=11, size_average=True, val_range=None, normalize=True):\n",
    "    device = img1.device\n",
    "    weights = torch.FloatTensor([0.0448, 0.2856, 0.3001, 0.2363, 0.1333]).to(device)\n",
    "    levels = weights.size()[0]\n",
    "    mssim = []\n",
    "    mcs = []\n",
    "    for _ in range(levels):\n",
    "        sim, cs = ssim(img1, img2, window_size=window_size, size_average=size_average, full=True, val_range=val_range)\n",
    "        mssim.append(sim)\n",
    "        mcs.append(cs)\n",
    "\n",
    "        img1 = F.avg_pool2d(img1, (2, 2))\n",
    "        img2 = F.avg_pool2d(img2, (2, 2))\n",
    "\n",
    "\n",
    "    mssim = torch.stack(mssim)\n",
    "    mcs = torch.stack(mcs)\n",
    "\n",
    "    # Normalize (to avoid NaNs during training unstable models, not compliant with original definition)\n",
    "    if normalize:\n",
    "        mssim = (mssim + 1) / 2\n",
    "        mcs = (mcs + 1) / 2\n",
    "\n",
    "    pow1 = mcs ** weights\n",
    "    pow2 = mssim ** weights\n",
    "    # From Matlab implementation https://ece.uwaterloo.ca/~z70wang/research/iwssim/\n",
    "    output = torch.prod(pow1[:-1] * pow2[-1])\n",
    "    return output\n",
    "\n",
    "\n",
    "class MSSSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True, channel=3):\n",
    "        super(MSSSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = channel\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        # TODO: store window between calls if possible\n",
    "        return 1 - msssim(img1, img2, window_size=self.window_size, size_average=self.size_average, val_range=img1.max())\n",
    "    \n",
    "ssim_loss = SSIM()\n",
    "msssim_loss = MSSSIM()\n",
    "\n",
    "#mean square error (MSELoss)\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "lr = 1e-4\n",
    "wd = 0.0\n",
    "#optimiser\n",
    "#stochastic gradient descent (SGD)\n",
    "# optimiser = optim.SGD(model.parameters(), lr=lr)\n",
    "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
    "# optimiser = torch.optim.RMSprop(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0's loss: 0.7352574467658997\n",
      "Epoch 1's loss: 0.49669113755226135\n",
      "Epoch 2's loss: 0.41145452857017517\n",
      "Epoch 3's loss: 0.38900166749954224\n",
      "Epoch 4's loss: 0.37703055143356323\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    mean = []\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        target_img, input_img = data\n",
    "        input_img = input_img.to(device).unsqueeze(1)\n",
    "        target_img = target_img.to(device).unsqueeze(1)\n",
    "\n",
    "        output_img = model(input_img)\n",
    "        \n",
    "        loss = ssim_loss(output_img, target_img)\n",
    "        # loss = msssim_loss(output_img, target_img)\n",
    "        # loss = F.l1_loss(output_img, target_img)\n",
    "        # loss = loss_fn(output_img, target_img)\n",
    "        # loss = 0.84 * (1 - msssim_loss(output_img, target_img)) - 0.16 * F.l1_loss(output_img, target_img)\n",
    "        mean.append(loss)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    l = sum(mean)/len(mean)\n",
    "    print(\"Epoch {}'s loss: {}\".format(epoch, l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "#model.load_state_dict(torch.load('model_final.h5'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader for training set. It applies same to validation set as well\n",
    "val_dataset = MRIDataset(data_list['val'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=38, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_numpy(gt, pred):\n",
    "    return compare_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bham/modules/roots/neural-comp/2019-20/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5501071896680699 tensor(0.5507, device='cuda:0')\n",
      "0.5897546048952125 tensor(0.5954, device='cuda:0')\n",
      "0.5961351589454073 tensor(0.6018, device='cuda:0')\n",
      "0.6938788695043581 tensor(0.7011, device='cuda:0')\n",
      "0.598302807235098 tensor(0.6033, device='cuda:0')\n",
      "0.5970857088021189 tensor(0.6030, device='cuda:0')\n",
      "0.5072443215799641 tensor(0.5113, device='cuda:0')\n",
      "0.5724955347306269 tensor(0.5768, device='cuda:0')\n",
      "0.6578282980619549 tensor(0.6652, device='cuda:0')\n",
      "0.5908600671938099 tensor(0.5959, device='cuda:0')\n",
      "0.542855633543178 tensor(0.5476, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ssim_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for iteration, sample in enumerate(val_loader):\n",
    "    \n",
    "        img_gt, img_und = sample\n",
    "    \n",
    "        output_img = model(img_und.to(device).unsqueeze(1)).cpu().numpy().squeeze()\n",
    "        ssim_scores.append(ssim_numpy(img_gt.squeeze(1).numpy(), output_img))\n",
    "            \n",
    "numpy_ssims = np.array(ssim_scores)\n",
    "print(\"Mean:\", numpy_ssims.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for iteration, sample in enumerate(val_loader):\n",
    "    \n",
    "        img_gt, img_und = sample\n",
    "    \n",
    "        output_img = model(img_und.to(device).unsqueeze(1)).cpu().numpy().squeeze()\n",
    "        show_slices([img_gt.squeeze(1).numpy()[6], output_img[6]], [0, 1], cmap='gray')\n",
    "        \n",
    "        if iteration >= 3: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
